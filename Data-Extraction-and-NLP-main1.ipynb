{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2267a0-f71e-49d1-92dc-d8da982237d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf750293-6e61-41f6-ad06-e82241c4fe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/',\n",
       "  'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/',\n",
       "  'https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/',\n",
       "  'https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/',\n",
       "  'https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/',\n",
       "  'https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/',\n",
       "  'https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/',\n",
       "  'https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/',\n",
       "  'https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/',\n",
       "  'https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/'],\n",
       " 114)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Input.xlsx\")\n",
    "urls = list(df[\"URL\"])\n",
    "urls[:10], len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873b7e10-d213-4d8e-a5cc-8f75985f3e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lessons from the past: Some key learnings relevant to the coronavirus crisis\\n “The more you know about the past, the better prepared you are for the future.”Theodore RooseveltAs we speak, the world finds itself engulfed in one of its worst crises in recent times. The global COVID-19 pandemic has caused never-seen-before disruption in both public and economic life. Not only have factories shut down or supply chains abruptly stopped or millions of workers stranded, but festivals suspended, familie'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_web_data(url):\n",
    "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
    "    doc = requests.get(url)\n",
    "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
    "    title = soup.find(\"h1\")\n",
    "    article = soup.find_all(\"div\", {\"class\": class_[0]})\n",
    "    if article:\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    else:\n",
    "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    try:\n",
    "        start = res.index(\"Introduction\")\n",
    "        stop = res.index(\"Blackcoffer Insights\")\n",
    "    except:\n",
    "        start = 0\n",
    "        stop = -1\n",
    "    return title.text + \"\\n\" + res[start:stop]\n",
    "\n",
    "\n",
    "fetch_web_data(random.choice(urls))[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a3e27c-df79-4a93-b406-c8c90a682dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " 'ARIARY']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words():\n",
    "    StopWords_notNames = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file != \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            res = []\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
    "            if res != []:\n",
    "                StopWords_notNames.extend(res)\n",
    "\n",
    "    StopWords_Names = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file == \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
    "                    if res != None:\n",
    "                        StopWords_Names.append(res[0])\n",
    "\n",
    "    stop_words = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        corpus = open(f\"StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
    "        res = []\n",
    "        for txt in corpus:\n",
    "            if \"|\" in txt:\n",
    "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
    "                res.append(txt.strip())\n",
    "        if res != []:\n",
    "            stop_words.extend(res)\n",
    "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
    "\n",
    "    stop_words.extend(StopWords_notNames)\n",
    "    stop_words.extend(StopWords_Names)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "get_stop_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96a87522-f6a2-419c-bcfb-c7a42f690d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estimating impact COVID world work COVID unprecedented pandemic Can be possibility great leaders bill gates action rising year . corollary pandemic prodigious analysis Department Economic Social Affairs DESA stated COVID pandemic disrupting global supply chains international trade turn shrink global economy percent reversal previous forecast . percent growth. . million Americans filed unemployment claims economic downturn expected worst recession Great Depression stated IMF. India facing biggest'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_stop_words(text, personalwords=True):\n",
    "    stop_words=get_stop_words()\n",
    "    if personalwords == True:\n",
    "        stop_words.extend(nltk_sw.words(\"english\"))\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    cleaned_text = \" \".join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88410d94-9898-4d64-8e61-5e6419d36388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 54, -0.1978021978021978, 0.754653130287648)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(text):\n",
    "    def get_subjectivity_score(text):\n",
    "        num_words = len(text.split())\n",
    "        unique_words = len(set(text.split()))\n",
    "        subjectivity_score = unique_words / num_words\n",
    "        return subjectivity_score\n",
    "\n",
    "    def get_polarity_score(text):\n",
    "        positive_words = (\n",
    "            open(\"MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "        negative_words = (\n",
    "            open(\"MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word.lower() in positive_words:\n",
    "                positive_count += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_count += 1\n",
    "\n",
    "        polarity_score = (positive_count - negative_count) / (\n",
    "            positive_count + negative_count + 1\n",
    "        )\n",
    "        return polarity_score, positive_count, negative_count\n",
    "\n",
    "    subjectivity_score = get_subjectivity_score(text)\n",
    "    polarity_score, positive_count, negative_count = get_polarity_score(text)\n",
    "\n",
    "    return positive_count, negative_count, polarity_score, subjectivity_score\n",
    "\n",
    "\n",
    "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d1cc148-2178-49eb-8d8c-c666c894737b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,\n",
       " 10.076923076923077,\n",
       " 0.09732824427480916,\n",
       " 4.069700528479155,\n",
       " 10.076923076923077,\n",
       " 2.2118320610687023)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Analysis_of_readability(fetched_article):\n",
    "    sentences = fetched_article.replace(\" \", \"\").split(\".\")\n",
    "    tokens = fetched_article.split(\" \")\n",
    "    total_num_of_sentences = len(sentences)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    num_complex_words = 0\n",
    "    for token in sentences:\n",
    "        if syllables.estimate(token) > 2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
    "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "\n",
    "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
    "\n",
    "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
    "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
    "    SYLLABLE_PER_WORD\n",
    "\n",
    "    return (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    )\n",
    "\n",
    "\n",
    "Analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8654c7bd-5b9e-4990-8a8d-24d7011e6368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6.531578947368421)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_personal_pronouns(tokens):\n",
    "    personal_pronouns = [\n",
    "        \"I\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "    ]\n",
    "    num_personal_pronouns = sum(\n",
    "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
    "    )\n",
    "\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_chars / len(tokens)\n",
    "\n",
    "    return num_personal_pronouns, avg_word_length\n",
    "\n",
    "\n",
    "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords=False)\n",
    "\n",
    "res = re.findall(\"[A-Za-z]+\", corpus)\n",
    "get_personal_pronouns(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "605f4a46-3f88-4691-81f6-d2cb57d17649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/ Not Found....!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>22.474359</td>\n",
       "      <td>0.043925</td>\n",
       "      <td>9.007313</td>\n",
       "      <td>22.474359</td>\n",
       "      <td>77</td>\n",
       "      <td>8877</td>\n",
       "      <td>1.954934</td>\n",
       "      <td>3</td>\n",
       "      <td>7.204651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>55</td>\n",
       "      <td>32</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>20.085714</td>\n",
       "      <td>0.047653</td>\n",
       "      <td>8.053347</td>\n",
       "      <td>20.085714</td>\n",
       "      <td>67</td>\n",
       "      <td>5208</td>\n",
       "      <td>1.645092</td>\n",
       "      <td>17</td>\n",
       "      <td>6.315341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>62</td>\n",
       "      <td>33</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.694725</td>\n",
       "      <td>19.658824</td>\n",
       "      <td>0.050269</td>\n",
       "      <td>7.883637</td>\n",
       "      <td>19.658824</td>\n",
       "      <td>84</td>\n",
       "      <td>7651</td>\n",
       "      <td>1.912029</td>\n",
       "      <td>5</td>\n",
       "      <td>7.353524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.680108</td>\n",
       "      <td>19.626506</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>7.870983</td>\n",
       "      <td>19.626506</td>\n",
       "      <td>83</td>\n",
       "      <td>5859</td>\n",
       "      <td>1.686924</td>\n",
       "      <td>2</td>\n",
       "      <td>6.626316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>58</td>\n",
       "      <td>21</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.723042</td>\n",
       "      <td>20.890244</td>\n",
       "      <td>0.047869</td>\n",
       "      <td>8.375245</td>\n",
       "      <td>20.890244</td>\n",
       "      <td>82</td>\n",
       "      <td>7035</td>\n",
       "      <td>1.751313</td>\n",
       "      <td>9</td>\n",
       "      <td>6.855530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>0.805501</td>\n",
       "      <td>16.370370</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>6.571678</td>\n",
       "      <td>16.370370</td>\n",
       "      <td>52</td>\n",
       "      <td>4136</td>\n",
       "      <td>1.843891</td>\n",
       "      <td>3</td>\n",
       "      <td>6.848659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.635120</td>\n",
       "      <td>18.554217</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>7.442466</td>\n",
       "      <td>18.554217</td>\n",
       "      <td>80</td>\n",
       "      <td>6706</td>\n",
       "      <td>1.706494</td>\n",
       "      <td>20</td>\n",
       "      <td>6.614499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.225352</td>\n",
       "      <td>0.610248</td>\n",
       "      <td>15.671233</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>6.291570</td>\n",
       "      <td>15.671233</td>\n",
       "      <td>66</td>\n",
       "      <td>5100</td>\n",
       "      <td>1.799825</td>\n",
       "      <td>9</td>\n",
       "      <td>6.746154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.770026</td>\n",
       "      <td>22.064516</td>\n",
       "      <td>0.045322</td>\n",
       "      <td>8.843935</td>\n",
       "      <td>22.064516</td>\n",
       "      <td>31</td>\n",
       "      <td>3509</td>\n",
       "      <td>2.067251</td>\n",
       "      <td>5</td>\n",
       "      <td>7.810606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.112676</td>\n",
       "      <td>0.682836</td>\n",
       "      <td>15.953846</td>\n",
       "      <td>0.062681</td>\n",
       "      <td>6.406611</td>\n",
       "      <td>15.953846</td>\n",
       "      <td>65</td>\n",
       "      <td>4359</td>\n",
       "      <td>1.777242</td>\n",
       "      <td>2</td>\n",
       "      <td>6.962963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "URL_ID                                                                       \n",
       "37                  66              29        0.385417            0.676136   \n",
       "38                  55              32        0.261364            0.682997   \n",
       "39                  62              33        0.302083            0.694725   \n",
       "40                  61              19        0.518519            0.680108   \n",
       "41                  58              21        0.462500            0.723042   \n",
       "...                ...             ...             ...                 ...   \n",
       "146                 22              28       -0.117647            0.805501   \n",
       "147                 38              14        0.452830            0.635120   \n",
       "148                 27              43       -0.225352            0.610248   \n",
       "149                 35               7        0.651163            0.770026   \n",
       "150                 31              39       -0.112676            0.682836   \n",
       "\n",
       "        AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                                \n",
       "37                22.474359                     0.043925   9.007313   \n",
       "38                20.085714                     0.047653   8.053347   \n",
       "39                19.658824                     0.050269   7.883637   \n",
       "40                19.626506                     0.050952   7.870983   \n",
       "41                20.890244                     0.047869   8.375245   \n",
       "...                     ...                          ...        ...   \n",
       "146               16.370370                     0.058824   6.571678   \n",
       "147               18.554217                     0.051948   7.442466   \n",
       "148               15.671233                     0.057692   6.291570   \n",
       "149               22.064516                     0.045322   8.843935   \n",
       "150               15.953846                     0.062681   6.406611   \n",
       "\n",
       "        AVG NUMBER OF WORDS PER SENTENCE   COMPLEX WORD COUNT  WORD COUNT   \\\n",
       "URL_ID                                                                       \n",
       "37                             22.474359                   77         8877   \n",
       "38                             20.085714                   67         5208   \n",
       "39                             19.658824                   84         7651   \n",
       "40                             19.626506                   83         5859   \n",
       "41                             20.890244                   82         7035   \n",
       "...                                  ...                  ...          ...   \n",
       "146                            16.370370                   52         4136   \n",
       "147                            18.554217                   80         6706   \n",
       "148                            15.671233                   66         5100   \n",
       "149                            22.064516                   31         3509   \n",
       "150                            15.953846                   65         4359   \n",
       "\n",
       "        SYLLABLE PER WORD   PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "URL_ID                                                          \n",
       "37                1.954934                  3         7.204651  \n",
       "38                1.645092                 17         6.315341  \n",
       "39                1.912029                  5         7.353524  \n",
       "40                1.686924                  2         6.626316  \n",
       "41                1.751313                  9         6.855530  \n",
       "...                    ...                ...              ...  \n",
       "146               1.843891                  3         6.848659  \n",
       "147               1.706494                 20         6.614499  \n",
       "148               1.799825                  9         6.746154  \n",
       "149               2.067251                  5         7.810606  \n",
       "150               1.777242                  2         6.962963  \n",
       "\n",
       "[111 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_r = []\n",
    "url_r = []\n",
    "pos_score_r = []\n",
    "neg_score_r = []\n",
    "Polarity_Score_r = []\n",
    "Polarity_Score_r = []\n",
    "Subjectivity_Score_r = []\n",
    "Average_Sentence_Length_r = []\n",
    "Percentage_of_Complex_words_r = []\n",
    "Fog_Index_r = []\n",
    "Average_Number_of_Words_Per_Sentence_r = []\n",
    "num_complex_words_r = []\n",
    "total_num_of_words_r = []\n",
    "SYLLABLE_PER_WORD_r = []\n",
    "num_personal_pronouns_r = []\n",
    "avg_word_length_r = []\n",
    "\n",
    "\n",
    "# Iterating through URLS\n",
    "for n in range(len(urls)):\n",
    "    try:\n",
    "        # fetch_web_data\n",
    "        fetched_article = fetch_web_data(urls[n])\n",
    "    except:\n",
    "        print(f\"Page {urls[n]} Not Found....!\")\n",
    "        continue\n",
    "    index = df.iloc[n]\n",
    "    id_ = index[0]\n",
    "    url_ = index[1]\n",
    "\n",
    "    # clean_stop_words\n",
    "    tokens = clean_stop_words(fetched_article)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(tokens)\n",
    "\n",
    "    (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    ) = Analysis_of_readability(fetched_article)\n",
    "\n",
    "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
    "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
    "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
    "\n",
    "    # Appending obtained variables into respective lists\n",
    "    id_r.append(id_)\n",
    "    url_r.append(url_)\n",
    "    pos_score_r.append(pos_score)\n",
    "    neg_score_r.append(neg_score)\n",
    "    Polarity_Score_r.append(Polarity_Score)\n",
    "    Subjectivity_Score_r.append(Subjectivity_Score)\n",
    "    Average_Sentence_Length_r.append(Average_Sentence_Length)\n",
    "    Percentage_of_Complex_words_r.append(Percentage_of_Complex_words)\n",
    "    Fog_Index_r.append(Fog_Index)\n",
    "    Average_Number_of_Words_Per_Sentence_r.append(Average_Number_of_Words_Per_Sentence)\n",
    "    num_complex_words_r.append(num_complex_words)\n",
    "    total_num_of_words_r.append(total_num_of_words)\n",
    "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
    "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
    "    avg_word_length_r.append(avg_word_length)\n",
    "\n",
    "\n",
    "output = {\n",
    "    \"URL_ID\": id_r,\n",
    "    \"POSITIVE SCORE\": pos_score_r,\n",
    "    \"NEGATIVE SCORE\": neg_score_r,\n",
    "    \"POLARITY SCORE\": Polarity_Score_r,\n",
    "    \"SUBJECTIVITY SCORE\": Subjectivity_Score_r,\n",
    "    \"AVG SENTENCE LENGTH\": Average_Sentence_Length_r,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": Percentage_of_Complex_words_r,\n",
    "    \"FOG INDEX\": Fog_Index_r,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": Average_Number_of_Words_Per_Sentence_r,\n",
    "    \" COMPLEX WORD COUNT\": num_complex_words_r,\n",
    "    \"WORD COUNT \": total_num_of_words_r,\n",
    "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
    "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
    "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
    "}\n",
    "\n",
    "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e736f9-9090-48a4-8610-3b77cdc0cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output Data Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b002402-992a-43be-9fa8-056f903ae4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
